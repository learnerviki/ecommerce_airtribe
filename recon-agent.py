import ollama
import subprocess
import time
#from typing import string


previous_messages = []
tool_calls = []
#tools_output = []

system_prompt = """
You are a tool-driven web reconnaissance agent.

IMPORTANT:
- You have ONLY ONE tool: execute_commands
- execute_commands accepts ONE argument: cmd (string)
- You MUST pass the FULL shell command as a string
- You DO NOT have access to subfinder or httpx as separate tools

Your ONLY tasks:
1. Run subfinder via execute_commands.
2. Run httpx via execute_commands.
4. Stop once httpx is executed.

Decision Making:
1. Understand how many tools have been called by verifying the ToolCall

Example:

If subfinder is executed, you will see something like:
[ToolCall(function=Function(name='execute_commands', arguments={'cmd': 'subfinder -d bing.com -silent -all -o subfinder.txt'}))]

EXAMPLE:

User: scan example.com

Action: execute_commands
Action Input:

subfinder -d example.com -silent -all -o subfinder.txt


Action: execute_commands
Action Input:

httpx -td -o http.txt < subfinder.txt


Final:
We are finished
"""

def start_ollama(context_window=None):
    """
    Start ollama server 
    """

    
    if context_window:
        print("[+] Ollama Started with context window {}".format(context_window))
        subprocess.Popen(f"OLLAMA_CONTEXT_LENGTH={context_window} ollama serve",shell=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(4)
    else:
        print("[+] Ollama Started with context window")
        subprocess.Popen(f"ollama serve",shell=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(4)

    #return None

def stop_ollama():
    """
    Stop ollama server
    """
    print("[+] Ollama Stopped")
    subprocess.call("pkill ollama",shell=True)
    time.sleep(2)
    #return None

def execute_commands(cmd: str) -> str:
    """
    Use this function to execute system commands. Useful in calling tools for web reconnaissance like subfinder and httpx
    """
    return subprocess.check_output(cmd,shell=True).decode('utf-8')

start_ollama("4000")

llm_output = ollama.chat(
  'llama3.1:8b',
  messages=[
    {'role': 'system', 'content': system_prompt},
    {'role': 'user', 'content': input(">> ")}
    ],
  tools=[
    execute_commands
    ], # Actual function reference
)

stop_ollama()

available_functions = {
  'execute_commands':execute_commands
}

previous_messages.append(llm_output)


def execute_tools(response):
    print("[+] Execute tools functions")
    start_ollama("3000")
    #print(response[-1])
    for tool in response[-1].message.tool_calls or []:
        function_to_call = available_functions.get(tool.function.name)
        if function_to_call:
            tool_output = function_to_call(**tool.function.arguments)

            print(tool_output)
            #tools_output.append({"arg": **tool.function.arguments,"output":tool_output})
            #print('Tool Called')
        else:
            pass
            #print('Function not found:', tool.function.name)
    previous_messages.append(response)
    stop_ollama()


def parse_tool_calls(previous_messages):
    for i in previous_messages:
        #print(i)
        try:
            #print(f"Parser tool calls, extracting tools: {i[0].message.tool_calls}")
            #print(i)
            tool_calls.append(i.message.tool_calls[0])
        except Exception as e:
            print(e)
            print()
            continue
    print(f"Tool calls: {tool_calls}")
    return tool_calls


def plan_next_tool(response):
    print("[+] Plan next tool function")
    start_ollama("4000")
    parse_tool_calls(response)
    if "httpx" in str(tool_calls):
        previous_messages.append(llm_output)
        #stop_ollama()
        return True
    user_prompt =  f"So far, we have called these tools: {str(tool_calls)}."
    #print(user_prompt)
    llm_output_2 = ollama.chat(
    'llama3.1:8b',
    messages=[
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': user_prompt}
        ],
    tools=[
        execute_commands
        ] # Actual function reference
    )
    previous_messages.append(llm_output_2)
    stop_ollama()
    return None


def extract_filename(tool_call):
    print("[+] Extract Filename function")
    start_ollama("1000")
    tool_call = tool_call[-1]
    print(tool_call)
    system_prompt = "Extract the filename from the text mentioned below which is generated by httpx. Just return the filename without any additional data"
    llm_output_2 = ollama.chat(
    'llama3.1:8b',
    messages=[
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': f"{tool_call}"}
        ],
    )
    with open(llm_output_2.message.content,"r") as text:
        out = text.readlines()
    stop_ollama()
    return out

def prioritize_target():
    print("[+] Prioritize Target function")
    system_prompt = """
    You will receive raw httpx output directly as user input. The input represents multiple web targets, typically one per line, and may include metadata such as URL, HTTP status code, server headers, title, technology stack, response time, port, IP address, or CDN indicators.

Your task is to analyze the provided content and prioritize the targets based on their potential security value.

Assign higher priority to targets that are more likely to expose vulnerabilities. Factors that increase priority include outdated or legacy technologies, verbose or uncommon server banners, missing or weak security-related headers, non-standard ports or services, misconfigurations, error-prone responses such as 5xx or unusual 4xx codes, direct IP access, lack of CDN or WAF protection, and self-hosted or custom technology stacks.

Assign lower priority to targets that appear well protected or low risk, such as those behind major CDNs or WAFs, fully managed platforms, modern and well-maintained technology stacks, or targets with minimal exposed metadata.

You must infer prioritization strictly from the provided content. Do not assume additional information and do not fabricate missing details.

After analysis, produce a ranked list of targets ordered from highest to lowest priority. For each target, provide a short technical justification explaining why it was prioritized. Explanations should be concise, factual, and focused on reconnaissance value.

Be deterministic, structured, and consistent in your reasoning. If certain metadata is missing or unclear, explicitly state that and adjust the priority accordingly. Do not attempt exploitation or suggest attack payloads. Your role is limited to reconnaissance triage and target prioritization.
    """
    http_content = extract_filename(tool_calls)
    start_ollama("4000")
    llm_output_2 = ollama.chat(
    'llama3.1:8b',
    messages=[
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': f"{http_content}"}
        ],stream=True
    )
    in_thinking = False
    content = ''
    thinking = ''
    for chunk in llm_output_2:
        if chunk.message.thinking:
            if not in_thinking:
                in_thinking = True
                print('Thinking:\n', end='', flush=True)
            print(chunk.message.thinking, end='', flush=True)
            # accumulate the partial thinking 
            thinking += chunk.message.thinking
        elif chunk.message.content:
            if in_thinking:
                in_thinking = False
                print('\n\nAnswer:\n', end='', flush=True)
            print(chunk.message.content, end='', flush=True)
            # accumulate the partial content
            content += chunk.message.content
    stop_ollama()


#execute_tools(previous_messages)
while True:
    if previous_messages[-1].message.tool_calls or []:
        execute_tools(previous_messages)
        if plan_next_tool(previous_messages):
            break
        continue
    print(previous_messages[-1].message.content)
    break

prioritize_target()

